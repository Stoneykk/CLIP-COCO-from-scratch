# CLIP-COCO-from-scratch

A minimal CLIP (Contrastive Language-Image Pre-training) implementation from scratch using ResNet-50 and frozen DistilBERT, trained on COCO Caption dataset for multimodal learning research and education.

## ğŸ¯ Project Overview

This project implements a simplified version of CLIP that learns to associate images with their corresponding text descriptions through contrastive learning. The model consists of:

- **Image Encoder**: ResNet-50 (pre-trained on ImageNet)
- **Text Encoder**: DistilBERT (frozen parameters)
- **Projection Head**: Linear layers to map both encoders to a common embedding space
- **Loss Function**: InfoNCE contrastive loss

## ğŸ“¦ Project Structure

```
CLIP-COCO-from-scratch/
â”œâ”€â”€ data/                        # Training data (images + text)
â”‚   â””â”€â”€ coco/                    # COCO caption dataset structure
â”‚       â”œâ”€â”€ images/             
â”‚       â””â”€â”€ captions.json       
â”œâ”€â”€ models/                      # Model definitions
â”‚   â”œâ”€â”€ image_encoder.py        # ResNet50 image encoder
â”‚   â”œâ”€â”€ text_encoder.py         # DistilBERT text encoder (frozen)
â”‚   â””â”€â”€ clip_model.py           # Complete CLIP model
â”œâ”€â”€ datasets/                    # Dataset processing
â”‚   â””â”€â”€ coco_dataset.py         # COCO dataset loader
â”œâ”€â”€ train/                       # Training utilities
â”‚   â””â”€â”€ train_clip.py           # Training loop
â”œâ”€â”€ utils/                       # Utility functions
â”‚   â”œâ”€â”€ tokenizer.py            # Text tokenizer
â”‚   â”œâ”€â”€ metrics.py              # Evaluation metrics
â”‚   â””â”€â”€ logger.py               # Logging utilities
â”œâ”€â”€ configs/                     # Configuration files
â”‚   â””â”€â”€ config.yaml             # Hyperparameters
â”œâ”€â”€ tests/                       # Test files
â”‚   â””â”€â”€ test_forward_pass.py    # Unit tests
â””â”€â”€ main.py                     # Training entry point
```

## ğŸš€ Quick Start

### Installation

```bash
git clone https://github.com/Stoneykk/CLIP-COCO-from-scratch.git
cd CLIP-COCO-from-scratch
pip install -r requirements.txt
```

### Training

```bash
python main.py
```

## ğŸ”§ Development Status

ğŸš§ **Under Development** - Currently completed:
- âœ… Project structure setup
- ğŸ”„ Model implementation (in progress)
- â³ Dataset preparation
- â³ Training pipeline
- â³ Evaluation metrics

## ğŸ“‹ Requirements

- Python 3.8+
- PyTorch 1.9.0+
- Transformers 4.20.0+
- CUDA (optional, for GPU acceleration)

## ğŸ¤ Contributing

This is an educational project for learning CLIP implementation. Feel free to contribute improvements or report issues!

## ğŸ“„ License

MIT License - see LICENSE file for details.

## ğŸ™ Acknowledgments

- OpenAI CLIP for the original paper and inspiration
- Hugging Face Transformers for DistilBERT
- PyTorch team for the deep learning framework
- COCO dataset creators
