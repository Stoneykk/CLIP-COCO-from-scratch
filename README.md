# CLIP-COCO-from-scratch

A minimal CLIP (Contrastive Language-Image Pre-training) implementation from scratch using ResNet-50 and frozen DistilBERT, trained on COCO Caption dataset for multimodal learning.

## ğŸ¯ Project Overview

This project implements a simplified version of CLIP that learns to associate images with their corresponding text descriptions through contrastive learning. The model consists of:

- **Image Encoder**: ResNet-50 (pre-trained on ImageNet)
- **Text Encoder**: DistilBERT (LoRA+PEFT)
- **Projection Head**: Linear layers to map both encoders to a common embedding space
- **Loss Function**: InfoNCE contrastive loss

## ğŸ“¦ Project Structure

```
CLIP-COCO-from-scratch/
â”œâ”€â”€ data/                        # Training data (images + text)
â”‚   â””â”€â”€ coco/                    # COCO caption dataset structure
â”‚       â”œâ”€â”€ images/             
â”‚       â””â”€â”€ captions.json       
â”œâ”€â”€ models/                      # Model definitions
â”‚   â”œâ”€â”€ image_encoder.py        # ResNet50 image encoder
â”‚   â”œâ”€â”€ text_encoder.py         # DistilBERT text encoder (frozen)
â”‚   â””â”€â”€ clip_model.py           # Complete CLIP model
â”œâ”€â”€ datasets/                    # Dataset processing
â”‚   â””â”€â”€ coco_dataset.py         # COCO dataset loader
â”œâ”€â”€ train/                       # Training utilities
â”‚   â””â”€â”€ train_clip.py           # Training loop
â”œâ”€â”€ utils/                       # Utility functions
â”‚   â”œâ”€â”€ tokenizer.py            # Text tokenizer
â”‚   â”œâ”€â”€ metrics.py              # Evaluation metrics
â”‚   â””â”€â”€ logger.py               # Logging utilities
â”œâ”€â”€ configs/                     # Configuration files
â”‚   â””â”€â”€ config.yaml             # Hyperparameters
â”œâ”€â”€ tests/                       # Test files
â”‚   â””â”€â”€ test_forward_pass.py    # Unit tests
â””â”€â”€ main.py                     # Training entry point
```

## ğŸš€ Quick Start

### Installation

```bash
git clone https://github.com/Stoneykk/CLIP-COCO-from-scratch.git
cd CLIP-COCO-from-scratch
pip install -r requirements.txt
```

### Training

```bash
python main.py
```

## ğŸ”§ Development Status

ğŸš§ **Under Development** - Currently completed:

### âœ… Phase 1: Project Initialization & Core Modules (COMPLETED)
- âœ… Project structure setup
- âœ… ResNet-50 image encoder implementation
- âœ… Frozen DistilBERT text encoder implementation  
- âœ… Complete CLIP model with projection heads

### ğŸ”„ Phase 2: Data Preparation & Validation Pipeline (IN PROGRESS)
- â³ Tokenizer implementation
- â³ COCO dataset loader
- â³ DataLoader setup

### â³ Phase 3: Training & Loss Functions
- â³ InfoNCE loss implementation
- â³ Training forward pass
- â³ Optimizer and training loop

### â³ Phase 4: Validation & Evaluation
- â³ Recall@K metrics
- â³ Test scripts

### â³ Phase 5: Main Entry & Visualization
- â³ Main training script
- â³ Logging and visualization

## ğŸ“‹ Requirements

- Python 3.8+
- PyTorch 1.9.0+
- Transformers 4.20.0+
- CUDA (optional, for GPU acceleration)


## ğŸ™ Acknowledgments

- OpenAI CLIP for the original paper and inspiration
- Hugging Face Transformers for DistilBERT
- PyTorch team for the deep learning framework
- COCO dataset creators
